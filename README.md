# RunAnywhere React Native Starter App

A comprehensive starter app demonstrating the capabilities of the [RunAnywhere SDK](https://www.npmjs.com/org/runanywhere) - a privacy-first, on-device AI SDK for React Native.

![RunAnywhere](https://img.shields.io/badge/RunAnywhere-0.16.10-00D9FF)
![React Native](https://img.shields.io/badge/React%20Native-0.76.5-61DAFB)
![Platforms](https://img.shields.io/badge/Platforms-iOS%20%7C%20Android-green)

## âœ¨ Features

This starter app showcases four main capabilities of the RunAnywhere SDK:

### ğŸ’¬ Chat (LLM Text Generation)
- Streaming text generation with token-by-token output
- Performance metrics (tokens/second, total tokens)
- Cancel generation mid-stream
- Suggested prompts for quick testing
- Beautiful chat UI with message bubbles

### ğŸ¤ Speech-to-Text (STT)
- Real-time audio recording
- On-device transcription using Whisper models
- Audio level visualization
- Transcription history
- Privacy-first: all processing happens on device

### ğŸ”Š Text-to-Speech (TTS)
- Neural voice synthesis with Piper TTS
- Adjustable speech rate (0.5x - 2.0x)
- Sample texts for quick testing
- Audio playback controls
- High-quality, natural-sounding voices

### âœ¨ Voice Pipeline (Voice Agent)
- Full voice assistant experience
- Seamless integration: Speak â†’ Transcribe â†’ Generate â†’ Speak
- Real-time status updates
- Conversation history
- Complete end-to-end voice interaction

## ğŸ“¦ SDK Packages Used

This app uses three RunAnywhere packages:

| Package | Purpose | NPM |
|---------|---------|-----|
| `@runanywhere/core` | Core SDK with infrastructure | [View on NPM](https://www.npmjs.com/package/@runanywhere/core) |
| `@runanywhere/llamacpp` | LLM backend (LlamaCpp) | [View on NPM](https://www.npmjs.com/package/@runanywhere/llamacpp) |
| `@runanywhere/onnx` | STT/TTS/VAD backend (ONNX) | [View on NPM](https://www.npmjs.com/package/@runanywhere/onnx) |

## ğŸš€ Getting Started

### Prerequisites

- **Node.js** 18 or higher
- **React Native CLI** development environment ([setup guide](https://reactnative.dev/docs/environment-setup))
- **iOS:** Xcode 14+, CocoaPods, macOS
- **Android:** Android Studio, JDK 17+, NDK 27+
- **Physical device recommended** for best performance (AI models run slowly on simulators)

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/RunanywhereAI/react-native-starter-app.git
   cd react-native-starter-app
   ```

2. **Install dependencies**
   ```bash
   npm install
   ```
   > **Note:** This runs `patch-package` automatically via postinstall to apply necessary compatibility fixes.

3. **iOS Setup**
   ```bash
   cd ios
   pod install
   cd ..
   ```

4. **Run the app**

   **For iOS:**
   ```bash
   npx react-native run-ios
   ```

   **For Android:**
   ```bash
   npx react-native run-android
   ```

### Running on Physical Android Device

When running on a physical Android device, you need to set up port forwarding for the Metro bundler:

```bash
# Connect your device via USB and verify it's detected
adb devices

# Set up port forwarding (required for each USB session)
adb reverse tcp:8081 tcp:8081

# Start Metro bundler in one terminal
npx react-native start

# Run the app in another terminal
npx react-native run-android
```

> **Tip:** If you see "Could not connect to development server", run `adb reverse tcp:8081 tcp:8081` again.

### iOS Permissions

The app requires microphone access. Permissions are already configured in `ios/RunAnywhereStarter/Info.plist`:

```xml
<key>NSMicrophoneUsageDescription</key>
<string>This app needs microphone access for speech recognition and voice agent features</string>
<key>NSSpeechRecognitionUsageDescription</key>
<string>This app uses on-device speech recognition to transcribe your voice</string>
```

### Android Permissions

Required permissions are configured in `android/app/src/main/AndroidManifest.xml`:

```xml
<uses-permission android:name="android.permission.INTERNET" />
<uses-permission android:name="android.permission.RECORD_AUDIO" />
<uses-permission android:name="android.permission.WRITE_EXTERNAL_STORAGE" />
<uses-permission android:name="android.permission.READ_EXTERNAL_STORAGE" />
```

## ğŸ—ï¸ Architecture

```
src/
â”œâ”€â”€ App.tsx                      # Main app entry, SDK initialization
â”œâ”€â”€ theme/
â”‚   â””â”€â”€ colors.ts               # Color palette and theme
â”œâ”€â”€ services/
â”‚   â””â”€â”€ ModelService.tsx        # Model management (download, load, state)
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ FeatureCard.tsx         # Home screen feature cards
â”‚   â”œâ”€â”€ ModelLoaderWidget.tsx   # Model download/load UI
â”‚   â”œâ”€â”€ ChatMessageBubble.tsx   # Chat message UI
â”‚   â””â”€â”€ AudioVisualizer.tsx     # Audio level visualization
â”œâ”€â”€ screens/
â”‚   â”œâ”€â”€ HomeScreen.tsx          # Main navigation screen
â”‚   â”œâ”€â”€ ChatScreen.tsx          # LLM chat interface
â”‚   â”œâ”€â”€ SpeechToTextScreen.tsx  # STT interface
â”‚   â”œâ”€â”€ TextToSpeechScreen.tsx  # TTS interface
â”‚   â””â”€â”€ VoicePipelineScreen.tsx # Voice agent interface
â””â”€â”€ navigation/
    â””â”€â”€ types.ts                # Navigation type definitions
```

## ğŸ¤– Default Models

The app comes preconfigured with these models:

| Model | Purpose | Size | Source |
|-------|---------|------|--------|
| SmolLM2 360M Q8_0 | Text generation | ~400MB | HuggingFace |
| Sherpa ONNX Whisper Tiny EN | Speech recognition | ~80MB | RunAnywhere |
| Piper TTS (US English) | Voice synthesis | ~100MB | RunAnywhere |

## ğŸ¨ Customization

### Using Different Models

You can modify `src/services/ModelService.tsx` to use different models:

```typescript
// LLM Model - Example with a larger model
await LlamaCpp.addModel({
  id: 'qwen2-1.5b-q4',
  name: 'Qwen2 1.5B Q4',
  url: 'https://huggingface.co/...',
  memoryRequirement: 1500000000,
});

// STT Model - Example with multilingual support
await Onnx.addModel({
  id: 'whisper-small-multi',
  name: 'Whisper Small Multilingual',
  url: 'https://...',
  modality: ModelCategory.speechRecognition,
});
```

### Theming

The app uses a custom dark theme defined in `src/theme/colors.ts`. You can customize:

```typescript
export const AppColors = {
  primaryDark: '#0A0E1A',
  accentCyan: '#00D9FF',
  accentViolet: '#8B5CF6',
  // ... more colors
};
```

## ğŸ”’ Privacy

All AI processing happens **on-device**. No data is sent to external servers. The models are downloaded once and stored locally on the device.

- âœ… No internet required after model download
- âœ… All inference runs locally
- âœ… Your conversations never leave your device
- âœ… No API keys or cloud services needed

## ğŸ› Troubleshooting

### "Could not connect to development server" (Android)
This happens on physical Android devices because they can't reach `localhost` on your computer.

```bash
# Set up port forwarding
adb reverse tcp:8081 tcp:8081

# Verify Metro is running
curl http://localhost:8081/status  # Should return "packager-status:running"
```

### CMake Error: "add_subdirectory given source which is not an existing directory"
This happens when codegen hasn't run yet. Simply run the build again:

```bash
cd android && ./gradlew assembleDebug
```

The second run will succeed as codegen completes.

### Models not downloading
- Check your internet connection
- Ensure sufficient storage space (models can be 100MB-1GB)
- Check iOS/Android permissions
- Clear app data and try again

### Microphone not working
- Grant microphone permission in device settings
- Restart the app after granting permission
- On Android, check if permission is granted in AndroidManifest.xml

### Low performance
- Smaller models (like SmolLM2 360M) work better on mobile devices
- Close other apps to free up memory
- Use quantized models (Q4/Q8) for better performance
- Ensure you're running on a physical device (simulators are slow)

### Build errors
- Clear cache: `cd android && ./gradlew clean` or `cd ios && rm -rf Pods Podfile.lock`
- Reinstall dependencies: `rm -rf node_modules && npm install`
- For iOS: `cd ios && pod install --repo-update`

### Patches not applied
If you see build errors related to `react-native-nitro-modules`, ensure patches are applied:

```bash
npx patch-package
```

This should run automatically via `postinstall`, but you can run it manually if needed.

## ğŸ“š Documentation

- [RunAnywhere SDK Documentation](https://docs.runanywhere.ai)
- [React Native Documentation](https://reactnative.dev)
- [API Reference](https://docs.runanywhere.ai/api)

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

## ğŸ“„ License

This starter app is provided under the MIT License. The RunAnywhere SDK is licensed under the [RunAnywhere License](https://runanywhere.ai/license).

For commercial licensing inquiries, contact: san@runanywhere.ai

## ğŸ†˜ Support

- **GitHub Issues**: [Report bugs](https://github.com/RunanywhereAI/runanywhere-sdks/issues)
- **Email**: san@runanywhere.ai
- **Documentation**: [runanywhere.ai](https://runanywhere.ai)
- **Discord**: [Join our community](https://discord.gg/runanywhere)

## ğŸ¯ Next Steps

1. **Explore the code**: Check out each screen to understand how the SDK works
2. **Try different models**: Swap in your own models to see what works best
3. **Build your app**: Use this as a foundation for your own AI-powered app
4. **Share feedback**: Let us know what you think and what features you'd like to see

## â­ Acknowledgments

Built with:
- [React Native](https://reactnative.dev)
- [React Navigation](https://reactnavigation.org)
- [React Native Reanimated](https://docs.swmansion.com/react-native-reanimated)
- [React Native Linear Gradient](https://github.com/react-native-linear-gradient/react-native-linear-gradient)

Special thanks to the open-source community and the RunAnywhere team!

---

Made with â¤ï¸ by the RunAnywhere team
